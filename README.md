# Gradient-Descend-along-with-its-Variants Python-Implemenation

In this Repository I have implemented Gradient Descend along with its other variants that are generally used in deep learning in Python.


## Table of Content:

- Gradient Descend
- Stochastic Gradient Descent (SGD)
- Mini batch Gradient Descent (SGD)
- Momentum based Gradient Descent (SGD)
- Adagrad (short for adaptive gradient)
- Adelta
- Adam(Adaptive Gradient Descend)

![](/images/EDA_1.png)


**Gradient Descend**

![](/images/gr.png)


**Mini batch Gradient Descent (SGD)**

![](/images/mini.png)


**Momentum based Gradient Descent (SGD)**

![](/images/mom_plot.png)


**Adagrad (short for adaptive gradient)**

![](/images/adagrad.png)


**Adelta**

![](/images/adadelta.png)


**Adam(Adaptive Gradient Descend)**

![](/images/adam.png)
